{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d602e75d",
   "metadata": {},
   "source": [
    "# 0. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d97e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.layers import (LSTM, Dense, Concatenate, Attention, Dropout, Softmax,\n",
    "                                     Input, Flatten, Activation, Bidirectional, Permute, multiply, \n",
    "                                     ConvLSTM2D, MaxPooling3D, TimeDistributed, Conv2D, MaxPooling2D)\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "tf.autograph.set_verbosity(1)\n",
    "\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f470e2",
   "metadata": {},
   "source": [
    "# 1. Keypoints using MP Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20cde117",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "716e9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False                 \n",
    "    results = model.process(image)                \n",
    "    image.flags.writeable = True                   \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd7ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ebe952",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "FPS = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, pose)\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        draw_landmarks(image, results)               \n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb32f3",
   "metadata": {},
   "source": [
    "# 2. Extract Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a81823f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = []\n",
    "if results.pose_landmarks is not None:\n",
    "    for res in results.pose_landmarks.landmark:\n",
    "        test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "        pose.append(test)\n",
    "else:\n",
    "    print(\"No pose landmarks found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd92eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_landmarks = len(landmarks)\n",
    "num_values = len(test)\n",
    "num_input_values = num_landmarks*num_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dad5b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f4d1079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b907e8",
   "metadata": {},
   "source": [
    "# 3. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddcaecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nithinthangallapelly\\Desktop\\project\\data\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = os.path.join(os. getcwd(),'data') \n",
    "print(DATA_PATH)\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "actions = np.array(['curl', 'press', 'squat'])\n",
    "num_classes = len(actions)\n",
    "no_sequences = 0\n",
    "sequence_length = FPS*1\n",
    "start_folder = 161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fed6b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:     \n",
    "    for sequence in range(start_folder,no_sequences+start_folder):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))  \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7622b573",
   "metadata": {},
   "source": [
    "# 4. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d224561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41b81490",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    for idx, action in enumerate(actions):\n",
    "        for sequence in range(start_folder, start_folder+no_sequences):\n",
    "            for frame_num in range(sequence_length):\n",
    "                ret, frame = cap.read()\n",
    "                image, results = mediapipe_detection(frame, pose)\n",
    "                try:\n",
    "                    landmarks = results.pose_landmarks.landmark\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "                draw_landmarks(image, results) \n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    \n",
    "                    cv2.putText(image, 'Collecting {} Video # {}'.format(action, sequence), (15,30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 8, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting {} Video # {}'.format(action, sequence), (15,30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, colors[idx], 4, cv2.LINE_AA)\n",
    "                    \n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(1000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting {} Video # {}'.format(action, sequence), (15,30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 8, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting {} Video # {}'.format(action, sequence), (15,30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, colors[idx], 4, cv2.LINE_AA)\n",
    "                \n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b016129",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ec993",
   "metadata": {},
   "source": [
    "# 5. Preprocess Data and Create Labels/Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cad528c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'curl': 0, 'press': 1, 'squat': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0add3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):         \n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)  \n",
    "            \n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed4bbd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab459ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 30, 132) (180, 3)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ac49993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 30, 132) (162, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ae03d",
   "metadata": {},
   "source": [
    "# 6. Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "912f3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping(monitor='val_loss', min_delta=5e-4, patience=10, verbose=0, mode='min')\n",
    "lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=0, mode='min')\n",
    "chkpt_callback = ModelCheckpoint(filepath=DATA_PATH, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                 save_weights_only=False, mode='min', save_freq=1)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "batch_size = 32\n",
    "max_epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca0cad",
   "metadata": {},
   "source": [
    "## 6a. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "730f987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = f\"ExerciseRecognition-LSTM-{int(time.time())}\"\n",
    "log_dir = os.path.join(os.getcwd(), 'logs', NAME,'')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "callbacks = [tb_callback, es_callback, lr_callback, chkpt_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae7595c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 30, 128)           133632    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 30, 256)           394240    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               197120    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 749,955\n",
      "Trainable params: 749,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(LSTM(128, return_sequences=True, activation='relu', input_shape=(sequence_length, num_input_values)))\n",
    "lstm.add(LSTM(256, return_sequences=True, activation='relu'))\n",
    "lstm.add(LSTM(128, return_sequences=False, activation='relu'))\n",
    "lstm.add(Dense(128, activation='relu'))\n",
    "lstm.add(Dense(64, activation='relu'))\n",
    "lstm.add(Dense(actions.shape[0], activation='softmax'))\n",
    "print(lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a10e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5/5 [==============================] - 7s 353ms/step - loss: 1034446.7500 - categorical_accuracy: 0.3101 - val_loss: 610.8690 - val_categorical_accuracy: 0.3030 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 1679.5549 - categorical_accuracy: 0.3411 - val_loss: 1014.2176 - val_categorical_accuracy: 0.3030 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 1600.0239 - categorical_accuracy: 0.3411 - val_loss: 261.2590 - val_categorical_accuracy: 0.3030 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 316.3133 - categorical_accuracy: 0.3488 - val_loss: 644.4307 - val_categorical_accuracy: 0.3030 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 500.5238 - categorical_accuracy: 0.3333 - val_loss: 228.1315 - val_categorical_accuracy: 0.3939 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 497.6654 - categorical_accuracy: 0.3411 - val_loss: 596.2286 - val_categorical_accuracy: 0.3030 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 277.8269 - categorical_accuracy: 0.3488 - val_loss: 337.1821 - val_categorical_accuracy: 0.4848 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 396.3929 - categorical_accuracy: 0.3333 - val_loss: 132.8909 - val_categorical_accuracy: 0.3030 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 65.7856 - categorical_accuracy: 0.3721 - val_loss: 868.4843 - val_categorical_accuracy: 0.3030 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 455.9651 - categorical_accuracy: 0.3721 - val_loss: 147.3046 - val_categorical_accuracy: 0.3333 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 293.4781 - categorical_accuracy: 0.2946 - val_loss: 619.7474 - val_categorical_accuracy: 0.3030 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 1s 170ms/step - loss: 450.1720 - categorical_accuracy: 0.4031 - val_loss: 353.3330 - val_categorical_accuracy: 0.3030 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 470.6888 - categorical_accuracy: 0.3101 - val_loss: 360.4333 - val_categorical_accuracy: 0.3030 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 311.2996 - categorical_accuracy: 0.3566 - val_loss: 130.4886 - val_categorical_accuracy: 0.3030 - lr: 0.0020\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 76.0656 - categorical_accuracy: 0.3101 - val_loss: 68.6664 - val_categorical_accuracy: 0.3939 - lr: 0.0020\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 1s 167ms/step - loss: 74.0726 - categorical_accuracy: 0.3643 - val_loss: 64.5199 - val_categorical_accuracy: 0.3030 - lr: 0.0020\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 57.2823 - categorical_accuracy: 0.3256 - val_loss: 27.9566 - val_categorical_accuracy: 0.3030 - lr: 0.0020\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 17.3272 - categorical_accuracy: 0.2946 - val_loss: 23.4210 - val_categorical_accuracy: 0.3030 - lr: 0.0020\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 16.9752 - categorical_accuracy: 0.3566 - val_loss: 9.8221 - val_categorical_accuracy: 0.3939 - lr: 0.0020\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 14.4240 - categorical_accuracy: 0.3256 - val_loss: 354.2646 - val_categorical_accuracy: 0.3030 - lr: 0.0020\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 200.6769 - categorical_accuracy: 0.3643 - val_loss: 202.0443 - val_categorical_accuracy: 0.3939 - lr: 0.0020\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 1s 175ms/step - loss: 193.1107 - categorical_accuracy: 0.3178 - val_loss: 77.9406 - val_categorical_accuracy: 0.3030 - lr: 0.0020\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 108.6502 - categorical_accuracy: 0.3566 - val_loss: 115.1038 - val_categorical_accuracy: 0.3030 - lr: 0.0020\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 1s 165ms/step - loss: 76.2167 - categorical_accuracy: 0.3101 - val_loss: 45.2483 - val_categorical_accuracy: 0.3030 - lr: 0.0020\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 34.2983 - categorical_accuracy: 0.3256 - val_loss: 24.3940 - val_categorical_accuracy: 0.3030 - lr: 4.0000e-04\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 1s 191ms/step - loss: 18.2479 - categorical_accuracy: 0.3256 - val_loss: 16.1731 - val_categorical_accuracy: 0.3030 - lr: 4.0000e-04\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 1s 189ms/step - loss: 12.6133 - categorical_accuracy: 0.3256 - val_loss: 11.3353 - val_categorical_accuracy: 0.3030 - lr: 4.0000e-04\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 1s 186ms/step - loss: 9.6298 - categorical_accuracy: 0.3256 - val_loss: 9.3337 - val_categorical_accuracy: 0.3030 - lr: 4.0000e-04\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 7.3056 - categorical_accuracy: 0.3256 - val_loss: 5.7513 - val_categorical_accuracy: 0.3030 - lr: 4.0000e-04\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 1s 189ms/step - loss: 4.4697 - categorical_accuracy: 0.3178 - val_loss: 5.1704 - val_categorical_accuracy: 0.3030 - lr: 4.0000e-04\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 1s 176ms/step - loss: 4.3334 - categorical_accuracy: 0.3566 - val_loss: 4.2396 - val_categorical_accuracy: 0.3030 - lr: 4.0000e-04\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 3.2523 - categorical_accuracy: 0.3566 - val_loss: 2.9454 - val_categorical_accuracy: 0.3030 - lr: 4.0000e-04\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 2.0814 - categorical_accuracy: 0.3643 - val_loss: 1.6982 - val_categorical_accuracy: 0.3030 - lr: 4.0000e-04\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 1s 169ms/step - loss: 1.6080 - categorical_accuracy: 0.3256 - val_loss: 1.3517 - val_categorical_accuracy: 0.3030 - lr: 4.0000e-04\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 1s 185ms/step - loss: 1.3625 - categorical_accuracy: 0.3721 - val_loss: 1.3032 - val_categorical_accuracy: 0.3939 - lr: 4.0000e-04\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 1.3894 - categorical_accuracy: 0.3178 - val_loss: 1.1910 - val_categorical_accuracy: 0.3939 - lr: 4.0000e-04\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 1.2128 - categorical_accuracy: 0.3178 - val_loss: 1.0963 - val_categorical_accuracy: 0.3939 - lr: 4.0000e-04\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 1s 168ms/step - loss: 1.0994 - categorical_accuracy: 0.3798 - val_loss: 1.1047 - val_categorical_accuracy: 0.2727 - lr: 4.0000e-04\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 1s 166ms/step - loss: 1.1039 - categorical_accuracy: 0.3256 - val_loss: 1.0976 - val_categorical_accuracy: 0.3939 - lr: 4.0000e-04\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 1.1017 - categorical_accuracy: 0.3721 - val_loss: 1.0978 - val_categorical_accuracy: 0.3939 - lr: 4.0000e-04\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 1.1102 - categorical_accuracy: 0.3178 - val_loss: 1.1041 - val_categorical_accuracy: 0.3939 - lr: 4.0000e-04\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 1.1386 - categorical_accuracy: 0.3178 - val_loss: 1.1130 - val_categorical_accuracy: 0.3939 - lr: 4.0000e-04\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 1.1469 - categorical_accuracy: 0.3178 - val_loss: 1.1114 - val_categorical_accuracy: 0.3939 - lr: 8.0000e-05\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 1.1439 - categorical_accuracy: 0.3178 - val_loss: 1.1096 - val_categorical_accuracy: 0.3939 - lr: 8.0000e-05\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 1.1414 - categorical_accuracy: 0.3178 - val_loss: 1.1062 - val_categorical_accuracy: 0.3939 - lr: 8.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "5/5 [==============================] - 1s 194ms/step - loss: 1.1327 - categorical_accuracy: 0.3178 - val_loss: 1.0994 - val_categorical_accuracy: 0.3939 - lr: 8.0000e-05\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 1s 205ms/step - loss: 1.1133 - categorical_accuracy: 0.3178 - val_loss: 1.0989 - val_categorical_accuracy: 0.3939 - lr: 8.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b7a7351160>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "lstm.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs, validation_data=(X_val, y_val), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58c4d8",
   "metadata": {},
   "source": [
    "## 6b. LSTM + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6e12666",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = f\"ExerciseRecognition-AttnLSTM-{int(time.time())}\"\n",
    "log_dir = os.path.join(os.getcwd(), 'logs', NAME,'')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "callbacks = [tb_callback, es_callback, lr_callback, chkpt_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07591dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(inputs, time_steps):\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul') \n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5c2e2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 30, 132)]    0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 30, 512)      796672      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " permute (Permute)              (None, 512, 30)      0           ['bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 512, 30)      930         ['permute[0][0]']                \n",
      "                                                                                                  \n",
      " attention_vec (Permute)        (None, 30, 512)      0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " attention_mul (Multiply)       (None, 30, 512)      0           ['bidirectional[0][0]',          \n",
      "                                                                  'attention_vec[0][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 15360)        0           ['attention_mul[0][0]']          \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 512)          7864832     ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 3)            1539        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,663,973\n",
      "Trainable params: 8,663,973\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_UNITS = 256\n",
    "inputs = Input(shape=(sequence_length, num_input_values))\n",
    "lstm_out = Bidirectional(LSTM(HIDDEN_UNITS, return_sequences=True))(inputs)\n",
    "attention_mul = attention_block(lstm_out, sequence_length)\n",
    "attention_mul = Flatten()(attention_mul)\n",
    "x = Dense(2*HIDDEN_UNITS, activation='relu')(attention_mul)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(actions.shape[0], activation='softmax')(x)\n",
    "AttnLSTM = Model(inputs=[inputs], outputs=x)\n",
    "print(AttnLSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf2f988d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5/5 [==============================] - 8s 549ms/step - loss: 1.1004 - categorical_accuracy: 0.3566 - val_loss: 1.0999 - val_categorical_accuracy: 0.3030 - lr: 1.6000e-05\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 258ms/step - loss: 1.0951 - categorical_accuracy: 0.3721 - val_loss: 1.1003 - val_categorical_accuracy: 0.3030 - lr: 1.6000e-05\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 1s 259ms/step - loss: 1.0952 - categorical_accuracy: 0.4031 - val_loss: 1.0956 - val_categorical_accuracy: 0.3030 - lr: 1.6000e-05\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 259ms/step - loss: 1.0953 - categorical_accuracy: 0.3643 - val_loss: 1.0917 - val_categorical_accuracy: 0.6061 - lr: 1.6000e-05\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 258ms/step - loss: 1.0908 - categorical_accuracy: 0.4574 - val_loss: 1.0905 - val_categorical_accuracy: 0.5152 - lr: 1.6000e-05\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 254ms/step - loss: 1.0866 - categorical_accuracy: 0.5426 - val_loss: 1.0904 - val_categorical_accuracy: 0.6364 - lr: 1.6000e-05\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 254ms/step - loss: 1.0878 - categorical_accuracy: 0.4961 - val_loss: 1.0906 - val_categorical_accuracy: 0.4848 - lr: 1.6000e-05\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 259ms/step - loss: 1.0846 - categorical_accuracy: 0.5194 - val_loss: 1.0909 - val_categorical_accuracy: 0.6061 - lr: 1.6000e-05\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 1s 256ms/step - loss: 1.0800 - categorical_accuracy: 0.5659 - val_loss: 1.0882 - val_categorical_accuracy: 0.6061 - lr: 1.6000e-05\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 253ms/step - loss: 1.0805 - categorical_accuracy: 0.5969 - val_loss: 1.0849 - val_categorical_accuracy: 0.5758 - lr: 1.6000e-05\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 252ms/step - loss: 1.0773 - categorical_accuracy: 0.5349 - val_loss: 1.0834 - val_categorical_accuracy: 0.4242 - lr: 1.6000e-05\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 1s 258ms/step - loss: 1.0764 - categorical_accuracy: 0.5039 - val_loss: 1.0815 - val_categorical_accuracy: 0.5758 - lr: 1.6000e-05\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 1s 258ms/step - loss: 1.0700 - categorical_accuracy: 0.6124 - val_loss: 1.0796 - val_categorical_accuracy: 0.5455 - lr: 1.6000e-05\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 1s 260ms/step - loss: 1.0691 - categorical_accuracy: 0.6357 - val_loss: 1.0759 - val_categorical_accuracy: 0.5455 - lr: 1.6000e-05\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 1s 254ms/step - loss: 1.0650 - categorical_accuracy: 0.6357 - val_loss: 1.0725 - val_categorical_accuracy: 0.6061 - lr: 1.6000e-05\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 1s 254ms/step - loss: 1.0564 - categorical_accuracy: 0.6124 - val_loss: 1.0681 - val_categorical_accuracy: 0.5758 - lr: 1.6000e-05\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 1s 257ms/step - loss: 1.0570 - categorical_accuracy: 0.6434 - val_loss: 1.0631 - val_categorical_accuracy: 0.7273 - lr: 1.6000e-05\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 1s 257ms/step - loss: 1.0459 - categorical_accuracy: 0.7519 - val_loss: 1.0583 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 1s 254ms/step - loss: 1.0429 - categorical_accuracy: 0.7132 - val_loss: 1.0536 - val_categorical_accuracy: 0.6364 - lr: 1.6000e-05\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 1s 256ms/step - loss: 1.0324 - categorical_accuracy: 0.7597 - val_loss: 1.0484 - val_categorical_accuracy: 0.6061 - lr: 1.6000e-05\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 1s 254ms/step - loss: 1.0325 - categorical_accuracy: 0.6822 - val_loss: 1.0420 - val_categorical_accuracy: 0.6364 - lr: 1.6000e-05\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 1s 268ms/step - loss: 1.0230 - categorical_accuracy: 0.6434 - val_loss: 1.0375 - val_categorical_accuracy: 0.5758 - lr: 1.6000e-05\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 1s 271ms/step - loss: 1.0128 - categorical_accuracy: 0.5814 - val_loss: 1.0306 - val_categorical_accuracy: 0.5758 - lr: 1.6000e-05\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 1s 271ms/step - loss: 1.0002 - categorical_accuracy: 0.6202 - val_loss: 1.0189 - val_categorical_accuracy: 0.5455 - lr: 1.6000e-05\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 1s 260ms/step - loss: 0.9784 - categorical_accuracy: 0.6512 - val_loss: 1.0092 - val_categorical_accuracy: 0.5758 - lr: 1.6000e-05\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 1s 260ms/step - loss: 0.9765 - categorical_accuracy: 0.6434 - val_loss: 1.0016 - val_categorical_accuracy: 0.5455 - lr: 1.6000e-05\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 1s 294ms/step - loss: 0.9502 - categorical_accuracy: 0.6357 - val_loss: 0.9928 - val_categorical_accuracy: 0.5152 - lr: 1.6000e-05\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 1s 265ms/step - loss: 0.9420 - categorical_accuracy: 0.6202 - val_loss: 0.9782 - val_categorical_accuracy: 0.5455 - lr: 1.6000e-05\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 1s 256ms/step - loss: 0.9250 - categorical_accuracy: 0.6124 - val_loss: 0.9477 - val_categorical_accuracy: 0.5758 - lr: 1.6000e-05\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 1s 256ms/step - loss: 0.8820 - categorical_accuracy: 0.7442 - val_loss: 0.9300 - val_categorical_accuracy: 0.6667 - lr: 1.6000e-05\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 1s 254ms/step - loss: 0.8625 - categorical_accuracy: 0.7752 - val_loss: 0.9115 - val_categorical_accuracy: 0.6970 - lr: 1.6000e-05\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 1s 255ms/step - loss: 0.8648 - categorical_accuracy: 0.7132 - val_loss: 0.9109 - val_categorical_accuracy: 0.5758 - lr: 1.6000e-05\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 1s 257ms/step - loss: 0.8380 - categorical_accuracy: 0.7597 - val_loss: 0.8744 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 1s 252ms/step - loss: 0.8064 - categorical_accuracy: 0.8760 - val_loss: 0.8711 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 1s 254ms/step - loss: 0.7930 - categorical_accuracy: 0.7984 - val_loss: 0.8367 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 1s 252ms/step - loss: 0.7645 - categorical_accuracy: 0.8605 - val_loss: 0.8151 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 1s 261ms/step - loss: 0.7473 - categorical_accuracy: 0.8682 - val_loss: 0.7895 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 1s 253ms/step - loss: 0.7157 - categorical_accuracy: 0.8372 - val_loss: 0.7702 - val_categorical_accuracy: 0.7576 - lr: 1.6000e-05\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 1s 259ms/step - loss: 0.6885 - categorical_accuracy: 0.8760 - val_loss: 0.7608 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 1s 259ms/step - loss: 0.6772 - categorical_accuracy: 0.8760 - val_loss: 0.7255 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 1s 257ms/step - loss: 0.6481 - categorical_accuracy: 0.8915 - val_loss: 0.7108 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 1s 262ms/step - loss: 0.6266 - categorical_accuracy: 0.8605 - val_loss: 0.6941 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 1s 260ms/step - loss: 0.6066 - categorical_accuracy: 0.8760 - val_loss: 0.6699 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 1s 265ms/step - loss: 0.5942 - categorical_accuracy: 0.8372 - val_loss: 0.6529 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 1s 253ms/step - loss: 0.5725 - categorical_accuracy: 0.8837 - val_loss: 0.6388 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "5/5 [==============================] - 1s 252ms/step - loss: 0.5563 - categorical_accuracy: 0.8837 - val_loss: 0.6216 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 1s 253ms/step - loss: 0.5357 - categorical_accuracy: 0.8837 - val_loss: 0.6129 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 1s 255ms/step - loss: 0.5445 - categorical_accuracy: 0.9070 - val_loss: 0.6027 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 1s 255ms/step - loss: 0.5454 - categorical_accuracy: 0.9070 - val_loss: 0.5864 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 1s 256ms/step - loss: 0.5062 - categorical_accuracy: 0.9070 - val_loss: 0.5643 - val_categorical_accuracy: 0.8788 - lr: 1.6000e-05\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 1s 261ms/step - loss: 0.5085 - categorical_accuracy: 0.8992 - val_loss: 0.5576 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 1s 257ms/step - loss: 0.4952 - categorical_accuracy: 0.8450 - val_loss: 0.5640 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 1s 268ms/step - loss: 0.4680 - categorical_accuracy: 0.8682 - val_loss: 0.5569 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 1s 294ms/step - loss: 0.5065 - categorical_accuracy: 0.7829 - val_loss: 0.5817 - val_categorical_accuracy: 0.7576 - lr: 1.6000e-05\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.4926 - categorical_accuracy: 0.8605 - val_loss: 0.5174 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 1s 273ms/step - loss: 0.4734 - categorical_accuracy: 0.9147 - val_loss: 0.5173 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 1s 263ms/step - loss: 0.4261 - categorical_accuracy: 0.9147 - val_loss: 0.5047 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 2s 356ms/step - loss: 0.4423 - categorical_accuracy: 0.9070 - val_loss: 0.5032 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 2s 353ms/step - loss: 0.4191 - categorical_accuracy: 0.9147 - val_loss: 0.4953 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 1s 289ms/step - loss: 0.4031 - categorical_accuracy: 0.9302 - val_loss: 0.4752 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 1s 299ms/step - loss: 0.4126 - categorical_accuracy: 0.8992 - val_loss: 0.5228 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 1s 291ms/step - loss: 0.4440 - categorical_accuracy: 0.8605 - val_loss: 0.4832 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 2s 311ms/step - loss: 0.3789 - categorical_accuracy: 0.9147 - val_loss: 0.4856 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 1s 290ms/step - loss: 0.3871 - categorical_accuracy: 0.8992 - val_loss: 0.4663 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 2s 354ms/step - loss: 0.3578 - categorical_accuracy: 0.9225 - val_loss: 0.4414 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 2s 315ms/step - loss: 0.3504 - categorical_accuracy: 0.9302 - val_loss: 0.4344 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 1s 285ms/step - loss: 0.3512 - categorical_accuracy: 0.9147 - val_loss: 0.4410 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 2s 320ms/step - loss: 0.3676 - categorical_accuracy: 0.9302 - val_loss: 0.4274 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 2s 358ms/step - loss: 0.3382 - categorical_accuracy: 0.9225 - val_loss: 0.4167 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 2s 401ms/step - loss: 0.3284 - categorical_accuracy: 0.9380 - val_loss: 0.4132 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 2s 342ms/step - loss: 0.3265 - categorical_accuracy: 0.9302 - val_loss: 0.4245 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 2s 378ms/step - loss: 0.3272 - categorical_accuracy: 0.9147 - val_loss: 0.3984 - val_categorical_accuracy: 0.8788 - lr: 1.6000e-05\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 2s 488ms/step - loss: 0.3286 - categorical_accuracy: 0.9225 - val_loss: 0.3874 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 2s 396ms/step - loss: 0.2999 - categorical_accuracy: 0.9380 - val_loss: 0.4007 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 1s 254ms/step - loss: 0.3302 - categorical_accuracy: 0.9225 - val_loss: 0.4428 - val_categorical_accuracy: 0.7879 - lr: 1.6000e-05\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 1s 264ms/step - loss: 0.3147 - categorical_accuracy: 0.9070 - val_loss: 0.4035 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 1s 257ms/step - loss: 0.2998 - categorical_accuracy: 0.9302 - val_loss: 0.3859 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 1s 262ms/step - loss: 0.2864 - categorical_accuracy: 0.9457 - val_loss: 0.3774 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 1s 272ms/step - loss: 0.2788 - categorical_accuracy: 0.9380 - val_loss: 0.3730 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 2s 321ms/step - loss: 0.2853 - categorical_accuracy: 0.9302 - val_loss: 0.3689 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 3s 677ms/step - loss: 0.2700 - categorical_accuracy: 0.9380 - val_loss: 0.3769 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 2s 492ms/step - loss: 0.2580 - categorical_accuracy: 0.9535 - val_loss: 0.3705 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 2s 477ms/step - loss: 0.2627 - categorical_accuracy: 0.9380 - val_loss: 0.3593 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 2s 340ms/step - loss: 0.2363 - categorical_accuracy: 0.9535 - val_loss: 0.3690 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 2s 343ms/step - loss: 0.2853 - categorical_accuracy: 0.8915 - val_loss: 0.4280 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 2s 321ms/step - loss: 0.2731 - categorical_accuracy: 0.9147 - val_loss: 0.3505 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 2s 324ms/step - loss: 0.2556 - categorical_accuracy: 0.9380 - val_loss: 0.3691 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 2s 370ms/step - loss: 0.2576 - categorical_accuracy: 0.9380 - val_loss: 0.3429 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 2s 329ms/step - loss: 0.2313 - categorical_accuracy: 0.9612 - val_loss: 0.3641 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 2s 392ms/step - loss: 0.2308 - categorical_accuracy: 0.9612 - val_loss: 0.3503 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/200\n",
      "5/5 [==============================] - 2s 404ms/step - loss: 0.2325 - categorical_accuracy: 0.9612 - val_loss: 0.3367 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 2s 354ms/step - loss: 0.2323 - categorical_accuracy: 0.9612 - val_loss: 0.3601 - val_categorical_accuracy: 0.8788 - lr: 1.6000e-05\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 2s 314ms/step - loss: 0.2298 - categorical_accuracy: 0.9690 - val_loss: 0.3670 - val_categorical_accuracy: 0.8788 - lr: 1.6000e-05\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 2s 348ms/step - loss: 0.2332 - categorical_accuracy: 0.9612 - val_loss: 0.3453 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 1s 288ms/step - loss: 0.2244 - categorical_accuracy: 0.9612 - val_loss: 0.3176 - val_categorical_accuracy: 0.8788 - lr: 1.6000e-05\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 1s 271ms/step - loss: 0.2240 - categorical_accuracy: 0.9612 - val_loss: 0.3142 - val_categorical_accuracy: 0.8788 - lr: 1.6000e-05\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 1s 262ms/step - loss: 0.2235 - categorical_accuracy: 0.9612 - val_loss: 0.3254 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 1s 264ms/step - loss: 0.2191 - categorical_accuracy: 0.9612 - val_loss: 0.3196 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 1s 262ms/step - loss: 0.1990 - categorical_accuracy: 0.9690 - val_loss: 0.3146 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 1s 282ms/step - loss: 0.2028 - categorical_accuracy: 0.9690 - val_loss: 0.3158 - val_categorical_accuracy: 0.8485 - lr: 1.6000e-05\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 1s 266ms/step - loss: 0.1983 - categorical_accuracy: 0.9612 - val_loss: 0.3236 - val_categorical_accuracy: 0.8182 - lr: 1.6000e-05\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 1s 257ms/step - loss: 0.1909 - categorical_accuracy: 0.9690 - val_loss: 0.3201 - val_categorical_accuracy: 0.8182 - lr: 1.0000e-05\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 1s 256ms/step - loss: 0.1963 - categorical_accuracy: 0.9612 - val_loss: 0.3173 - val_categorical_accuracy: 0.8182 - lr: 1.0000e-05\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 1s 271ms/step - loss: 0.1852 - categorical_accuracy: 0.9690 - val_loss: 0.3159 - val_categorical_accuracy: 0.8182 - lr: 1.0000e-05\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 1s 271ms/step - loss: 0.1910 - categorical_accuracy: 0.9690 - val_loss: 0.3169 - val_categorical_accuracy: 0.8182 - lr: 1.0000e-05\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 1s 262ms/step - loss: 0.1891 - categorical_accuracy: 0.9690 - val_loss: 0.3189 - val_categorical_accuracy: 0.8182 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b7aa9405e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AttnLSTM.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "AttnLSTM.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs, validation_data=(X_val, y_val), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b89f67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LSTM': lstm, \n",
    "    'LSTM_Attention_128HUs': AttnLSTM, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928f612",
   "metadata": {},
   "source": [
    "# 7a. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a7647ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    save_dir = os.path.join(os.getcwd(), f\"{model_name}.h5\")\n",
    "    model.save(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fecf26",
   "metadata": {},
   "source": [
    "# 7b. Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed0114a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    load_dir = os.path.join(os.getcwd(), f\"{model_name}.h5\")\n",
    "    model.load_weights(load_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7747c6",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2101a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models.values():\n",
    "    res = model.predict(X_test, verbose=0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36c98e",
   "metadata": {},
   "source": [
    "# 9. Evaluations using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecf242d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = {}\n",
    "eval_results['confusion matrix'] = None\n",
    "eval_results['accuracy'] = None\n",
    "eval_results['precision'] = None\n",
    "eval_results['recall'] = None\n",
    "eval_results['f1 score'] = None\n",
    "\n",
    "confusion_matrices = {}\n",
    "classification_accuracies = {}   \n",
    "precisions = {}\n",
    "recalls = {}\n",
    "f1_scores = {} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6778f",
   "metadata": {},
   "source": [
    "## 9a. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fccbb90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM confusion matrix: \r\n",
      "[[[ 0 12]\n",
      "  [ 0  6]]\n",
      "\n",
      " [[10  0]\n",
      "  [ 8  0]]\n",
      "\n",
      " [[14  0]\n",
      "  [ 4  0]]]\n",
      "LSTM_Attention_128HUs confusion matrix: \r\n",
      "[[[12  0]\n",
      "  [ 1  5]]\n",
      "\n",
      " [[10  0]\n",
      "  [ 0  8]]\n",
      "\n",
      " [[13  1]\n",
      "  [ 0  4]]]\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    confusion_matrices[model_name] = multilabel_confusion_matrix(ytrue, yhat)\n",
    "    print(f\"{model_name} confusion matrix: {os.linesep}{confusion_matrices[model_name]}\")\n",
    "eval_results['confusion matrix'] = confusion_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c6dc5",
   "metadata": {},
   "source": [
    "## 9b. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e36146f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM classification accuracy = 33.333%\n",
      "LSTM_Attention_128HUs classification accuracy = 94.444%\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    classification_accuracies[model_name] = accuracy_score(ytrue, yhat)    \n",
    "    print(f\"{model_name} classification accuracy = {round(classification_accuracies[model_name]*100,3)}%\")\n",
    "eval_results['accuracy'] = classification_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33efa73a",
   "metadata": {},
   "source": [
    "## 9c. Precision, Recall, and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35067c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM weighted average precision = 0.778\n",
      "LSTM weighted average recall = 0.333\n",
      "LSTM weighted average f1-score = 0.167\n",
      "\n",
      "LSTM_Attention_128HUs weighted average precision = 0.956\n",
      "LSTM_Attention_128HUs weighted average recall = 0.944\n",
      "LSTM_Attention_128HUs weighted average f1-score = 0.945\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    report = classification_report(ytrue, yhat, target_names=actions, output_dict=True,zero_division=1)\n",
    "    \n",
    "    precisions[model_name] = report['weighted avg']['precision']\n",
    "    recalls[model_name] = report['weighted avg']['recall']\n",
    "    f1_scores[model_name] = report['weighted avg']['f1-score'] \n",
    "   \n",
    "    print(f\"{model_name} weighted average precision = {round(precisions[model_name],3)}\")\n",
    "    print(f\"{model_name} weighted average recall = {round(recalls[model_name],3)}\")\n",
    "    print(f\"{model_name} weighted average f1-score = {round(f1_scores[model_name],3)}\\n\")\n",
    "\n",
    " \n",
    "eval_results['precision'] = precisions\n",
    "eval_results['recall'] = recalls\n",
    "eval_results['f1 score'] = f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d39476",
   "metadata": {},
   "source": [
    "# 10. Choose Model to Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d72d0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttnLSTM\n",
    "model_name = 'AttnLSTM'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0015ce",
   "metadata": {},
   "source": [
    "# 11. Calculate Joint Angles & Count Reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f172932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26f357fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(landmarks, mp_pose, side, joint):\n",
    "    coord = getattr(mp_pose.PoseLandmark,side.upper()+\"_\"+joint.upper())\n",
    "    x_coord_val = landmarks[coord.value].x\n",
    "    y_coord_val = landmarks[coord.value].y\n",
    "    return [x_coord_val, y_coord_val]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f11273cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_joint_angle(image, angle, joint):\n",
    "    cv2.putText(image, str(int(angle)), \n",
    "                   tuple(np.multiply(joint, [640, 480]).astype(int)), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "                        )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b64050d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_reps(image, current_action, landmarks, mp_pose):\n",
    "    global curl_counter, press_counter, squat_counter, curl_stage, press_stage, squat_stage\n",
    "    \n",
    "    if current_action == 'curl':\n",
    "        shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        elbow = get_coordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "        wrist = get_coordinates(landmarks, mp_pose, 'left', 'wrist')\n",
    "        \n",
    "        angle = calculate_angle(shoulder, elbow, wrist)\n",
    "        # curl counter logic\n",
    "        if angle < 30:\n",
    "            curl_stage = \"up\" \n",
    "        if angle > 160 and curl_stage =='up':\n",
    "            curl_stage=\"down\"  \n",
    "            curl_counter +=1\n",
    "        press_stage = None\n",
    "        squat_stage = None\n",
    "        viz_joint_angle(image, angle, elbow)\n",
    "        # press counter Logic\n",
    "    elif current_action == 'press':\n",
    "        shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        elbow = get_coordinates(landmarks, mp_pose, 'left', 'elbow')\n",
    "        wrist = get_coordinates(landmarks, mp_pose, 'left', 'wrist')\n",
    "        elbow_angle = calculate_angle(shoulder, elbow, wrist)\n",
    "        shoulder2elbow_dist = abs(math.dist(shoulder,elbow))\n",
    "        shoulder2wrist_dist = abs(math.dist(shoulder,wrist))\n",
    "        if (elbow_angle > 130) and (shoulder2elbow_dist < shoulder2wrist_dist):\n",
    "            press_stage = \"up\"\n",
    "        if (elbow_angle < 50) and (shoulder2elbow_dist > shoulder2wrist_dist) and (press_stage =='up'):\n",
    "            press_stage='down'\n",
    "            press_counter += 1\n",
    "        curl_stage = None\n",
    "        squat_stage = None\n",
    "        viz_joint_angle(image, elbow_angle, elbow)\n",
    "        # Squat counter logic\n",
    "    elif current_action == 'squat':\n",
    "        left_shoulder = get_coordinates(landmarks, mp_pose, 'left', 'shoulder')\n",
    "        left_hip = get_coordinates(landmarks, mp_pose, 'left', 'hip')\n",
    "        left_knee = get_coordinates(landmarks, mp_pose, 'left', 'knee')\n",
    "        left_ankle = get_coordinates(landmarks, mp_pose, 'left', 'ankle')\n",
    "        right_shoulder = get_coordinates(landmarks, mp_pose, 'right', 'shoulder')\n",
    "        right_hip = get_coordinates(landmarks, mp_pose, 'right', 'hip')\n",
    "        right_knee = get_coordinates(landmarks, mp_pose, 'right', 'knee')\n",
    "        right_ankle = get_coordinates(landmarks, mp_pose, 'right', 'ankle')\n",
    "        \n",
    "        left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "        right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "        \n",
    "        left_hip_angle = calculate_angle(left_shoulder, left_hip, left_knee)\n",
    "        right_hip_angle = calculate_angle(right_shoulder, right_hip, right_knee)\n",
    "        \n",
    "        # Squat counter logic\n",
    "        thr = 165\n",
    "        if (left_knee_angle < thr) and (right_knee_angle < thr) and (left_hip_angle < thr) and (right_hip_angle < thr):\n",
    "            squat_stage = \"down\"\n",
    "        if (left_knee_angle > thr) and (right_knee_angle > thr) and (left_hip_angle > thr) and (right_hip_angle > thr) and (squat_stage =='down'):\n",
    "            squat_stage='up'\n",
    "            squat_counter += 1\n",
    "        curl_stage = None\n",
    "        press_stage = None\n",
    "        viz_joint_angle(image, left_knee_angle, left_knee)\n",
    "        viz_joint_angle(image, left_hip_angle, left_hip)\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5116ef6",
   "metadata": {},
   "source": [
    "# 12. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4775b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):        \n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6332bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = []\n",
    "predictions = []\n",
    "res = []\n",
    "threshold = 0.5 \n",
    "current_action = ''\n",
    "\n",
    "curl_counter = 0\n",
    "press_counter = 0\n",
    "squat_counter = 0\n",
    "curl_stage = None\n",
    "press_stage = None\n",
    "squat_stage = None\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('M','J','P','G') \n",
    "HEIGHT = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) \n",
    "WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "FPS = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "video_name = os.path.join(os.getcwd(),f\"{model_name}_real_time_test2.avi\")\n",
    "out = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*\"MJPG\"), FPS, (WIDTH,HEIGHT))\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image, results = mediapipe_detection(frame, pose)\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        keypoints = extract_keypoints(results)        \n",
    "        sequence.append(keypoints)      \n",
    "        sequence = sequence[-sequence_length:]\n",
    "              \n",
    "        if len(sequence) == sequence_length:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0), verbose=0)[0]           \n",
    "            predictions.append(np.argmax(res))\n",
    "            current_action = actions[np.argmax(res)]\n",
    "            confidence = np.max(res)\n",
    "            \n",
    "            if confidence < threshold:\n",
    "                current_action = ''\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                count_reps(\n",
    "                    image, current_action, landmarks, mp_pose)\n",
    "            except:\n",
    "                pass\n",
    "            cv2.rectangle(image, (0,0), (640, 40), colors[np.argmax(res)], -1)\n",
    "            cv2.putText(image, 'curl ' + str(curl_counter), (3,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'press ' + str(press_counter), (240,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, 'squat ' + str(squat_counter), (490,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "         \n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "        if ret == True:\n",
    "            out.write(image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af9980a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "80aa1d3f3a8cfb37a38c47373cc49a39149184c5fa770d709389b1b8782c1d85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
